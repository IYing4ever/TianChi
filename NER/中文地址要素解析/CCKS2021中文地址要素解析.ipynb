{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.10.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# install requirements: adaseq\n!pip install transformers seqeval modelscope\n!pip install adaseq --ignore-requires-python --no-deps","metadata":{"ExecutionIndicator":{"show":true},"tags":[],"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Looking in indexes: https://mirrors.aliyun.com/pypi/simple\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.25.1)\nCollecting seqeval\n  Downloading https://mirrors.aliyun.com/pypi/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.6/43.6 kB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hCollecting modelscope\n  Downloading https://mirrors.aliyun.com/pypi/packages/3c/33/553d775dd38932af489f6b9a2a192c60a099632494f9e9cc7e2e86ac8980/modelscope-1.4.2-py3-none-any.whl (4.2 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.2/4.2 MB\u001B[0m \u001B[31m3.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2022.10.31)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.6.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from transformers) (6.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.3)\nRequirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from modelscope) (1.10.1)\nRequirement already satisfied: yapf in /opt/conda/lib/python3.10/site-packages (from modelscope) (0.32.0)\nRequirement already satisfied: gast>=0.2.2 in /root/.local/lib/python3.10/site-packages (from modelscope) (0.4.0)\nCollecting oss2\n  Downloading https://mirrors.aliyun.com/pypi/packages/86/e7/017f8a5948d70e130815eebd14c25dfad916bd574590f2d7e046f19eee3f/oss2-2.17.0.tar.gz (259 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m259.5/259.5 kB\u001B[0m \u001B[31m13.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: Pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from modelscope) (9.4.0)\nRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from modelscope) (0.4.1)\nRequirement already satisfied: attrs in /opt/conda/lib/python3.10/site-packages (from modelscope) (22.1.0)\nCollecting pyarrow!=9.0.0,>=6.0.0\n  Downloading https://mirrors.aliyun.com/pypi/packages/bc/5b/cee81fdfbd963e08a50eda1500fc9142fa08a7dec5f7dcd9c7c2a33536b6/pyarrow-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m34.9/34.9 MB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hCollecting jsonplus\n  Downloading https://mirrors.aliyun.com/pypi/packages/d6/0a/0438f4131477fa827431137eec9c4bd11e7884065e5812dc989c80d91e82/jsonplus-0.8.0-py2.py3-none-any.whl (11 kB)\nRequirement already satisfied: addict in /opt/conda/lib/python3.10/site-packages (from modelscope) (2.4.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from modelscope) (65.5.0)\nCollecting datasets<=2.8.0,>=2.7.0\n  Downloading https://mirrors.aliyun.com/pypi/packages/24/57/6b07e4dc51479ae3e9bbc774af348b0307e2b66957ceae94d25e3f9d7dcf/datasets-2.8.0-py3-none-any.whl (452 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m452.9/452.9 kB\u001B[0m \u001B[31m2.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets<=2.8.0,>=2.7.0->modelscope) (1.5.3)\nCollecting xxhash\n  Downloading https://mirrors.aliyun.com/pypi/packages/32/c3/4d24d4868fab9d9c8980ce00f01e3302565b06e712129d7dda779f9bb714/xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m212.5/212.5 kB\u001B[0m \u001B[31m2.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hCollecting dill<0.3.7\n  Downloading https://mirrors.aliyun.com/pypi/packages/be/e3/a84bf2e561beed15813080d693b4b27573262433fced9c1d1fea59e60553/dill-0.3.6-py3-none-any.whl (110 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m110.5/110.5 kB\u001B[0m \u001B[31m11.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets<=2.8.0,>=2.7.0->modelscope) (3.8.4)\nCollecting responses<0.19\n  Downloading https://mirrors.aliyun.com/pypi/packages/79/f3/2b3a6dc5986303b3dd1bbbcf482022acb2583c428cd23f0b6d37b1a1a519/responses-0.18.0-py3-none-any.whl (38 kB)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets<=2.8.0,>=2.7.0->modelscope) (2023.3.0)\nCollecting multiprocess\n  Downloading https://mirrors.aliyun.com/pypi/packages/b8/0c/c26b346b41bb1f81ac921fa10074a9595c22e5f99cc89c0410fc4efd5df3/multiprocess-0.70.14-py310-none-any.whl (134 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.3/134.3 kB\u001B[0m \u001B[31m8.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.13)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\nRequirement already satisfied: sortedcontainers>=1.5.9 in /opt/conda/lib/python3.10/site-packages (from jsonplus->modelscope) (2.4.0)\nCollecting simplejson>=3.3.0\n  Downloading https://mirrors.aliyun.com/pypi/packages/08/3e/bd02a72fa11aae93af2333dbfd44d27ff1e91062ef1855d12ed124726725/simplejson-3.18.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m137.4/137.4 kB\u001B[0m \u001B[31m9.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.10/site-packages (from jsonplus->modelscope) (2.8.2)\nCollecting crcmod>=1.7\n  Downloading https://mirrors.aliyun.com/pypi/packages/6b/b0/e595ce2a2527e169c3bcd6c33d2473c1918e0b7f6826a043ca1245dd4e5b/crcmod-1.7.tar.gz (89 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m89.7/89.7 kB\u001B[0m \u001B[31m15.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: pycryptodome>=3.4.7 in /opt/conda/lib/python3.10/site-packages (from oss2->modelscope) (3.17)\nCollecting aliyun-python-sdk-kms>=2.4.1\n  Downloading https://mirrors.aliyun.com/pypi/packages/70/25/a361278473fb39a1f1aded563518706abad6a0e2d4104cae60b2de056028/aliyun_python_sdk_kms-2.16.0-py2.py3-none-any.whl (67 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.4/67.4 kB\u001B[0m \u001B[31m4.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting aliyun-python-sdk-core>=2.13.12\n  Downloading https://mirrors.aliyun.com/pypi/packages/55/5a/6eec6c6e78817e5ca2afee661f2bbb33dbcfa2ce09a2980b52223323bd2e/aliyun-python-sdk-core-2.13.36.tar.gz (440 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m440.5/440.5 kB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from oss2->modelscope) (1.16.0)\nCollecting jmespath<1.0.0,>=0.9.3\n  Downloading https://mirrors.aliyun.com/pypi/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\nRequirement already satisfied: cryptography>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->modelscope) (38.0.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=2.8.0,>=2.7.0->modelscope) (1.3.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=2.8.0,>=2.7.0->modelscope) (1.8.2)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=2.8.0,>=2.7.0->modelscope) (2.0.4)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=2.8.0,>=2.7.0->modelscope) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=2.8.0,>=2.7.0->modelscope) (4.0.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=2.8.0,>=2.7.0->modelscope) (1.3.3)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets<=2.8.0,>=2.7.0->modelscope) (2022.1)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2->modelscope) (1.15.1)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2->modelscope) (2.21)\nBuilding wheels for collected packages: seqeval, oss2, aliyun-python-sdk-core, crcmod\n  Building wheel for seqeval (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=c559064bcc62d3feecabd14a13bcc8021836f051053c333fae139cc098e8ea31\n  Stored in directory: /root/.cache/pip/wheels/3a/1b/f3/e58d19d62452680b7ac2b49c36aef791806f6274bd6652e29b\n  Building wheel for oss2 (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for oss2: filename=oss2-2.17.0-py3-none-any.whl size=112375 sha256=632d134a87a03a44d4fb02a189eb85ab1a322b95a22aec8f30c2b20c73d43ff0\n  Stored in directory: /root/.cache/pip/wheels/40/79/f3/3f4975a92cf91ceb1d1fb0ad07e116d2ce9c7ac2bce5fe477e\n  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.13.36-py3-none-any.whl size=533196 sha256=fa023cd3d501d28ade765496c6c1782f12013e7b630c8b3bcd470e603331a8f0\n  Stored in directory: /root/.cache/pip/wheels/54/9a/53/f26f0842908662f1f3aa7247e1686a16eec238b5eed90aec46\n  Building wheel for crcmod (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for crcmod: filename=crcmod-1.7-py3-none-any.whl size=18834 sha256=a82f2299dbda02daabe43eb1fbc15ac7fec0236d219ec53c35c40e429ee46c94\n  Stored in directory: /root/.cache/pip/wheels/aa/64/0d/d371a836ec27ec35ef16ce7b8dae4898f6e1ffb309268bfceb\nSuccessfully built seqeval oss2 aliyun-python-sdk-core crcmod\nInstalling collected packages: crcmod, xxhash, simplejson, pyarrow, jmespath, dill, responses, multiprocess, jsonplus, seqeval, aliyun-python-sdk-core, datasets, aliyun-python-sdk-kms, oss2, modelscope\nSuccessfully installed aliyun-python-sdk-core-2.13.36 aliyun-python-sdk-kms-2.16.0 crcmod-1.7 datasets-2.8.0 dill-0.3.6 jmespath-0.10.0 jsonplus-0.8.0 modelscope-1.4.2 multiprocess-0.70.14 oss2-2.17.0 pyarrow-11.0.0 responses-0.18.0 seqeval-1.2.2 simplejson-3.18.4 xxhash-3.2.0\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0mLooking in indexes: https://mirrors.aliyun.com/pypi/simple\nCollecting adaseq\n  Downloading https://mirrors.aliyun.com/pypi/packages/c8/36/0752eead5e6872078820d53ca8f07974380c725b7de2f48925a0893dad2e/adaseq-0.6.2-py3-none-any.whl (136 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m136.7/136.7 kB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hInstalling collected packages: adaseq\nSuccessfully installed adaseq-0.6.2\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m","output_type":"stream"}],"id":"459f6699-27e1-4664-86b1-7b8bbee90002"},{"cell_type":"code","source":"path = '/mnt/workspace/downloads/109339/'","metadata":{"trusted":true},"execution_count":20,"outputs":[],"id":"2341027c-9743-4d9b-a092-7951ef39a834"},{"cell_type":"code","source":"# process test file to conll format\nwith open(path + 'final_test.txt', 'r', encoding='utf8') as fin, \\\n     open(path + 'test.conll', 'w', encoding='utf8') as fout:\n    for line in fin:\n        guid, text = line.strip().split('\\u0001')\n        for token in text:\n            print(token, 'O', sep=' ', file=fout)\n        print('', file=fout)    ","metadata":{"tags":[],"trusted":true},"execution_count":21,"outputs":[],"id":"4d220d79-fb37-4494-b6a5-d24b5feaaba1"},{"cell_type":"code","source":"# prepare training configuration\nfrom modelscope.utils.config import Config\n\nconfig = Config.from_string(\"\"\"\nexperiment:\n  exp_dir: experiments/\n  exp_name: transformer_crf\n  seed: 42\n\ntask: named-entity-recognition\n\ndataset:\n  data_file:\n    train: /mnt/workspace/downloads/109339/train.txt\n    valid: /mnt/workspace/downloads/109339/dev.txt\n    test: /mnt/workspace/downloads/109339/test.conll\n  data_type: conll\n\npreprocessor:\n  type: sequence-labeling-preprocessor\n  max_length: 80\n\ndata_collator: SequenceLabelingDataCollatorWithPadding\n\nmodel:\n  type: sequence-labeling-model\n  embedder:\n    model_name_or_path: damo/nlp_raner_named-entity-recognition_chinese-base-news\n  dropout: 0.15\n  use_crf: true\n\ntrain:\n  max_epochs: 30\n  dataloader:\n    batch_size_per_gpu: 16\n  optimizer:\n    type: AdamW\n    lr: 5.0e-5\n    param_groups:\n      - regex: crf\n        lr: 5.0e-1\n  lr_scheduler:\n    type: StepLR\n    step_size: 2 \n    gamma: 0.8\n  hooks:\n    - type: TensorboardHook\n\nevaluation:\n  dataloader:\n    batch_size_per_gpu: 128\n  metrics:\n    - type: ner-metric\n    - type: ner-dumper\n      model_type: sequence_labeling\n      dump_format: conll\n\"\"\", file_format='.yaml')","metadata":{"ExecutionIndicator":{"show":true},"tags":[],"trusted":true},"execution_count":30,"outputs":[],"id":"01932905-df03-4808-bec8-5d9d0c7d1084"},{"cell_type":"code","source":"# initialize a trainer\nimport os\nfrom adaseq.commands.train import build_trainer_from_partial_objects\n\nwork_dir = 'experiments/transformer_crf'\nos.makedirs(work_dir, exist_ok=True)\n\ntrainer = build_trainer_from_partial_objects(\n    config,\n    work_dir=work_dir,\n    seed=42,\n    device='cuda:0'\n)\n\n# do training\ntrainer.train()\n\n# do testing\ntrainer.test()","metadata":{"ExecutionIndicator":{"show":true},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"2023-04-01 12:34:16,593 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /opt/conda/lib/python3.10/site-packages/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py\n2023-04-01 12:34:16,687 - WARNING - datasets.builder - Using custom data configuration default-2f858739a689202e\n2023-04-01 12:34:16,699 - WARNING - datasets.builder - Found cached dataset named_entity_recognition_dataset_builder (/root/.cache/huggingface/datasets/named_entity_recognition_dataset_builder/default-2f858739a689202e/0.0.0/db737b9bb893f20fb03d04403a30bf7c033256c212b7e9f0ebc6e9c958535c51)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 439.47it/s]","output_type":"stream"},{"name":"stdout","text":"2023-04-01 12:34:16,710 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['浙', '江', '杭', '州', '市', '江', '干', '区', '九', '堡', '镇', '三', '村', '村', '一', '区'], 'spans': [{'start': 0, 'end': 2, 'type': 'prov'}, {'start': 2, 'end': 5, 'type': 'city'}, {'start': 5, 'end': 8, 'type': 'district'}, {'start': 8, 'end': 11, 'type': 'town'}, {'start': 11, 'end': 14, 'type': 'community'}, {'start': 14, 'end': 16, 'type': 'poi'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}\n","output_type":"stream"},{"name":"stderr","text":"\nCounting labels by count_span_labels: 100%|██████████| 8856/8856 [00:05<00:00, 1550.12ex/s]\nCounting labels by count_span_labels: 100%|██████████| 1970/1970 [00:01<00:00, 1956.01ex/s]\n","output_type":"stream"},{"name":"stdout","text":"2023-04-01 12:34:23,894 - INFO - adaseq.data.preprocessors.sequence_labeling_preprocessor - label_to_id: {'O': 0, 'B-assist': 1, 'I-assist': 2, 'E-assist': 3, 'S-assist': 4, 'B-cellno': 5, 'I-cellno': 6, 'E-cellno': 7, 'S-cellno': 8, 'B-city': 9, 'I-city': 10, 'E-city': 11, 'S-city': 12, 'B-community': 13, 'I-community': 14, 'E-community': 15, 'S-community': 16, 'B-devzone': 17, 'I-devzone': 18, 'E-devzone': 19, 'S-devzone': 20, 'B-distance': 21, 'I-distance': 22, 'E-distance': 23, 'S-distance': 24, 'B-district': 25, 'I-district': 26, 'E-district': 27, 'S-district': 28, 'B-floorno': 29, 'I-floorno': 30, 'E-floorno': 31, 'S-floorno': 32, 'B-houseno': 33, 'I-houseno': 34, 'E-houseno': 35, 'S-houseno': 36, 'B-intersection': 37, 'I-intersection': 38, 'E-intersection': 39, 'S-intersection': 40, 'B-poi': 41, 'I-poi': 42, 'E-poi': 43, 'S-poi': 44, 'B-prov': 45, 'I-prov': 46, 'E-prov': 47, 'S-prov': 48, 'B-road': 49, 'I-road': 50, 'E-road': 51, 'S-road': 52, 'B-roadno': 53, 'I-roadno': 54, 'E-roadno': 55, 'S-roadno': 56, 'B-subpoi': 57, 'I-subpoi': 58, 'E-subpoi': 59, 'S-subpoi': 60, 'B-town': 61, 'I-town': 62, 'E-town': 63, 'S-town': 64, 'B-village_group': 65, 'I-village_group': 66, 'E-village_group': 67, 'S-village_group': 68}\n2023-04-01 12:34:35,276 - INFO - modelscope - Model revision not specified, use the latest revision: v1.0.0\n2023-04-01 12:34:37,000 - INFO - modelscope - Model revision not specified, use the latest revision: v1.0.0\n2023-04-01 12:34:37,190 - INFO - modelscope - initialize model from /root/.cache/modelscope/hub/damo/nlp_raner_named-entity-recognition_chinese-base-news\n2023-04-01 12:34:37,193 - WARNING - modelscope - The backbone transformer-crf is not registered in modelscope, try to import the backbone from hf transformers.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at /root/.cache/modelscope/hub/damo/nlp_raner_named-entity-recognition_chinese-base-news were not used when initializing BertModel: ['encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.embeddings.word_embeddings.weight', 'encoder.encoder.layer.7.attention.output.dense.bias', 'encoder.encoder.layer.5.attention.output.dense.bias', 'encoder.encoder.layer.6.attention.self.key.weight', 'encoder.encoder.layer.7.attention.self.key.weight', 'encoder.encoder.layer.10.attention.self.value.bias', 'encoder.encoder.layer.2.attention.self.key.weight', 'encoder.embeddings.position_embeddings.weight', 'encoder.encoder.layer.8.attention.self.query.weight', 'encoder.encoder.layer.6.attention.self.value.bias', 'encoder.encoder.layer.0.output.dense.weight', 'encoder.encoder.layer.2.output.LayerNorm.weight', 'encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.encoder.layer.2.attention.output.dense.weight', 'encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.encoder.layer.7.attention.self.query.weight', 'encoder.encoder.layer.5.output.LayerNorm.weight', 'encoder.encoder.layer.1.attention.self.key.bias', 'encoder.encoder.layer.8.output.dense.weight', 'encoder.encoder.layer.4.output.LayerNorm.weight', 'crf.transitions', 'encoder.encoder.layer.5.attention.self.value.weight', 'encoder.encoder.layer.11.attention.output.dense.weight', 'encoder.pooler.dense.weight', 'encoder.encoder.layer.9.output.LayerNorm.bias', 'encoder.encoder.layer.6.output.dense.weight', 'encoder.encoder.layer.8.attention.self.key.weight', 'encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.encoder.layer.3.intermediate.dense.bias', 'encoder.encoder.layer.0.attention.self.value.bias', 'encoder.encoder.layer.9.attention.output.dense.bias', 'encoder.encoder.layer.8.output.dense.bias', 'encoder.encoder.layer.7.attention.self.query.bias', 'encoder.encoder.layer.8.output.LayerNorm.bias', 'encoder.encoder.layer.5.attention.self.query.bias', 'encoder.encoder.layer.4.attention.self.query.weight', 'encoder.encoder.layer.7.output.LayerNorm.weight', 'encoder.encoder.layer.8.attention.self.query.bias', 'encoder.encoder.layer.7.attention.self.key.bias', 'encoder.encoder.layer.11.output.LayerNorm.weight', 'encoder.encoder.layer.2.intermediate.dense.weight', 'encoder.encoder.layer.7.intermediate.dense.weight', 'encoder.encoder.layer.1.output.dense.bias', 'encoder.encoder.layer.9.output.LayerNorm.weight', 'encoder.encoder.layer.5.attention.self.key.bias', 'encoder.encoder.layer.5.output.dense.weight', 'encoder.encoder.layer.8.attention.output.dense.bias', 'encoder.encoder.layer.3.attention.self.query.bias', 'encoder.encoder.layer.0.attention.self.query.bias', 'encoder.encoder.layer.1.intermediate.dense.bias', 'encoder.encoder.layer.7.attention.self.value.bias', 'encoder.encoder.layer.8.intermediate.dense.weight', 'encoder.encoder.layer.6.output.LayerNorm.bias', 'encoder.encoder.layer.11.intermediate.dense.bias', 'encoder.encoder.layer.5.output.LayerNorm.bias', 'encoder.encoder.layer.11.attention.self.key.bias', 'encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.encoder.layer.2.attention.self.value.weight', 'encoder.encoder.layer.4.intermediate.dense.weight', 'encoder.encoder.layer.11.attention.output.dense.bias', 'encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.encoder.layer.7.output.dense.weight', 'encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.encoder.layer.1.attention.self.value.weight', 'encoder.encoder.layer.0.attention.output.dense.weight', 'encoder.encoder.layer.10.attention.self.key.bias', 'encoder.encoder.layer.6.intermediate.dense.weight', 'encoder.encoder.layer.9.attention.self.value.weight', 'encoder.encoder.layer.1.attention.self.key.weight', 'encoder.embeddings.LayerNorm.weight', 'encoder.encoder.layer.6.attention.self.query.bias', 'encoder.encoder.layer.7.intermediate.dense.bias', 'encoder.encoder.layer.9.attention.self.query.bias', 'encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.encoder.layer.6.attention.output.dense.weight', 'encoder.encoder.layer.7.output.dense.bias', 'encoder.encoder.layer.8.attention.output.dense.weight', 'encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.encoder.layer.2.output.LayerNorm.bias', 'encoder.encoder.layer.0.attention.output.dense.bias', 'encoder.encoder.layer.0.output.LayerNorm.weight', 'encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.encoder.layer.0.attention.self.query.weight', 'encoder.encoder.layer.2.attention.self.query.bias', 'encoder.encoder.layer.3.output.LayerNorm.weight', 'encoder.encoder.layer.5.attention.self.query.weight', 'encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.encoder.layer.1.output.dense.weight', 'encoder.encoder.layer.2.attention.self.query.weight', 'encoder.encoder.layer.1.output.LayerNorm.weight', 'encoder.encoder.layer.11.attention.self.key.weight', 'encoder.encoder.layer.1.intermediate.dense.weight', 'encoder.encoder.layer.1.attention.self.value.bias', 'encoder.encoder.layer.3.output.dense.bias', 'encoder.encoder.layer.6.attention.output.dense.bias', 'encoder.encoder.layer.6.attention.self.key.bias', 'encoder.encoder.layer.5.attention.self.value.bias', 'encoder.encoder.layer.8.attention.self.key.bias', 'encoder.encoder.layer.10.output.dense.weight', 'encoder.encoder.layer.9.attention.self.query.weight', 'encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.encoder.layer.2.attention.output.dense.bias', 'encoder.encoder.layer.1.attention.self.query.bias', 'encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.encoder.layer.2.intermediate.dense.bias', 'encoder.encoder.layer.11.output.dense.bias', 'encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.encoder.layer.9.output.dense.weight', 'encoder.encoder.layer.9.output.dense.bias', 'encoder.encoder.layer.10.output.dense.bias', 'encoder.embeddings.LayerNorm.bias', 'encoder.encoder.layer.3.intermediate.dense.weight', 'encoder.encoder.layer.4.attention.self.value.bias', 'encoder.encoder.layer.10.intermediate.dense.weight', 'encoder.encoder.layer.3.attention.self.value.bias', 'encoder.encoder.layer.10.attention.self.value.weight', 'encoder.encoder.layer.4.attention.self.query.bias', 'linear.bias', 'encoder.encoder.layer.4.attention.self.key.bias', 'encoder.encoder.layer.4.intermediate.dense.bias', 'encoder.encoder.layer.11.attention.self.query.bias', 'encoder.encoder.layer.11.output.LayerNorm.bias', 'encoder.encoder.layer.4.attention.self.value.weight', 'encoder.encoder.layer.3.attention.self.query.weight', 'encoder.encoder.layer.10.attention.self.query.bias', 'encoder.encoder.layer.7.attention.self.value.weight', 'encoder.encoder.layer.0.attention.self.key.bias', 'encoder.encoder.layer.10.attention.self.query.weight', 'encoder.encoder.layer.6.attention.self.value.weight', 'encoder.embeddings.position_ids', 'linear.weight', 'encoder.encoder.layer.0.output.dense.bias', 'crf.start_transitions', 'encoder.encoder.layer.3.attention.self.key.weight', 'encoder.encoder.layer.7.output.LayerNorm.bias', 'encoder.encoder.layer.11.attention.self.query.weight', 'encoder.encoder.layer.11.output.dense.weight', 'encoder.encoder.layer.0.intermediate.dense.bias', 'encoder.encoder.layer.6.output.dense.bias', 'encoder.encoder.layer.10.attention.output.dense.bias', 'encoder.pooler.dense.bias', 'encoder.encoder.layer.2.attention.self.key.bias', 'encoder.encoder.layer.0.attention.self.key.weight', 'encoder.encoder.layer.0.output.LayerNorm.bias', 'encoder.encoder.layer.8.attention.self.value.weight', 'encoder.encoder.layer.8.attention.self.value.bias', 'encoder.encoder.layer.9.attention.self.value.bias', 'encoder.encoder.layer.10.attention.self.key.weight', 'encoder.encoder.layer.2.output.dense.weight', 'encoder.encoder.layer.8.intermediate.dense.bias', 'encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.encoder.layer.7.attention.output.dense.weight', 'encoder.encoder.layer.10.output.LayerNorm.weight', 'encoder.encoder.layer.5.intermediate.dense.weight', 'encoder.encoder.layer.5.intermediate.dense.bias', 'encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.encoder.layer.4.output.dense.weight', 'encoder.encoder.layer.3.output.dense.weight', 'encoder.encoder.layer.4.attention.self.key.weight', 'encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.encoder.layer.3.output.LayerNorm.bias', 'encoder.encoder.layer.1.attention.self.query.weight', 'encoder.encoder.layer.9.attention.self.key.weight', 'crf.end_transitions', 'encoder.encoder.layer.8.output.LayerNorm.weight', 'encoder.encoder.layer.10.intermediate.dense.bias', 'encoder.encoder.layer.10.output.LayerNorm.bias', 'encoder.encoder.layer.6.attention.self.query.weight', 'encoder.encoder.layer.9.attention.self.key.bias', 'encoder.encoder.layer.3.attention.self.value.weight', 'encoder.encoder.layer.0.intermediate.dense.weight', 'encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.encoder.layer.5.output.dense.bias', 'encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.encoder.layer.1.output.LayerNorm.bias', 'encoder.embeddings.token_type_embeddings.weight', 'encoder.encoder.layer.1.attention.output.dense.bias', 'encoder.encoder.layer.5.attention.self.key.weight', 'encoder.encoder.layer.1.attention.output.dense.weight', 'encoder.encoder.layer.4.attention.output.dense.bias', 'encoder.encoder.layer.4.attention.output.dense.weight', 'encoder.encoder.layer.4.output.LayerNorm.bias', 'encoder.encoder.layer.3.attention.self.key.bias', 'encoder.encoder.layer.3.attention.output.dense.bias', 'encoder.encoder.layer.6.intermediate.dense.bias', 'encoder.encoder.layer.9.intermediate.dense.weight', 'encoder.encoder.layer.10.attention.output.dense.weight', 'encoder.encoder.layer.9.attention.output.dense.weight', 'encoder.encoder.layer.0.attention.self.value.weight', 'encoder.encoder.layer.6.output.LayerNorm.weight', 'encoder.encoder.layer.11.attention.self.value.weight', 'encoder.encoder.layer.9.intermediate.dense.bias', 'encoder.encoder.layer.11.attention.self.value.bias', 'encoder.encoder.layer.3.attention.output.dense.weight', 'encoder.encoder.layer.4.output.dense.bias', 'encoder.encoder.layer.2.output.dense.bias', 'encoder.encoder.layer.5.attention.output.dense.weight', 'encoder.encoder.layer.11.intermediate.dense.weight', 'encoder.encoder.layer.2.attention.self.value.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at /root/.cache/modelscope/hub/damo/nlp_raner_named-entity-recognition_chinese-base-news and are newly initialized: ['encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.self.value.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"** build_dataset error log: 'sequence-labeling-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'\n** build_dataset error log: 'sequence-labeling-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'\n2023-04-01 12:34:46,600 - INFO - modelscope - ==========================Training Config Start==========================\n2023-04-01 12:34:46,602 - INFO - modelscope - {\n    \"data_collator\": \"SequenceLabelingDataCollatorWithPadding\",\n    \"dataset\": {\n        \"data_file\": {\n            \"test\": \"/mnt/workspace/downloads/109339/test.conll\",\n            \"train\": \"/mnt/workspace/downloads/109339/train.txt\",\n            \"valid\": \"/mnt/workspace/downloads/109339/dev.txt\"\n        },\n        \"data_type\": \"conll\"\n    },\n    \"evaluation\": {\n        \"dataloader\": {\n            \"batch_size_per_gpu\": 128,\n            \"workers_per_gpu\": 0,\n            \"shuffle\": false\n        },\n        \"metrics\": [\n            {\n                \"type\": \"ner-metric\"\n            },\n            {\n                \"dump_format\": \"conll\",\n                \"model_type\": \"sequence_labeling\",\n                \"type\": \"ner-dumper\"\n            }\n        ],\n        \"period\": {}\n    },\n    \"experiment\": {\n        \"exp_dir\": \"experiments/\",\n        \"exp_name\": \"transformer_crf\",\n        \"seed\": 42\n    },\n    \"model\": {\n        \"dropout\": 0.15,\n        \"embedder\": {\n            \"model_name_or_path\": \"damo/nlp_raner_named-entity-recognition_chinese-base-news\",\n            \"type\": \"transformer-embedder\"\n        },\n        \"id_to_label\": {\n            \"0\": \"O\",\n            \"1\": \"B-assist\",\n            \"2\": \"I-assist\",\n            \"3\": \"E-assist\",\n            \"4\": \"S-assist\",\n            \"5\": \"B-cellno\",\n            \"6\": \"I-cellno\",\n            \"7\": \"E-cellno\",\n            \"8\": \"S-cellno\",\n            \"9\": \"B-city\",\n            \"10\": \"I-city\",\n            \"11\": \"E-city\",\n            \"12\": \"S-city\",\n            \"13\": \"B-community\",\n            \"14\": \"I-community\",\n            \"15\": \"E-community\",\n            \"16\": \"S-community\",\n            \"17\": \"B-devzone\",\n            \"18\": \"I-devzone\",\n            \"19\": \"E-devzone\",\n            \"20\": \"S-devzone\",\n            \"21\": \"B-distance\",\n            \"22\": \"I-distance\",\n            \"23\": \"E-distance\",\n            \"24\": \"S-distance\",\n            \"25\": \"B-district\",\n            \"26\": \"I-district\",\n            \"27\": \"E-district\",\n            \"28\": \"S-district\",\n            \"29\": \"B-floorno\",\n            \"30\": \"I-floorno\",\n            \"31\": \"E-floorno\",\n            \"32\": \"S-floorno\",\n            \"33\": \"B-houseno\",\n            \"34\": \"I-houseno\",\n            \"35\": \"E-houseno\",\n            \"36\": \"S-houseno\",\n            \"37\": \"B-intersection\",\n            \"38\": \"I-intersection\",\n            \"39\": \"E-intersection\",\n            \"40\": \"S-intersection\",\n            \"41\": \"B-poi\",\n            \"42\": \"I-poi\",\n            \"43\": \"E-poi\",\n            \"44\": \"S-poi\",\n            \"45\": \"B-prov\",\n            \"46\": \"I-prov\",\n            \"47\": \"E-prov\",\n            \"48\": \"S-prov\",\n            \"49\": \"B-road\",\n            \"50\": \"I-road\",\n            \"51\": \"E-road\",\n            \"52\": \"S-road\",\n            \"53\": \"B-roadno\",\n            \"54\": \"I-roadno\",\n            \"55\": \"E-roadno\",\n            \"56\": \"S-roadno\",\n            \"57\": \"B-subpoi\",\n            \"58\": \"I-subpoi\",\n            \"59\": \"E-subpoi\",\n            \"60\": \"S-subpoi\",\n            \"61\": \"B-town\",\n            \"62\": \"I-town\",\n            \"63\": \"E-town\",\n            \"64\": \"S-town\",\n            \"65\": \"B-village_group\",\n            \"66\": \"I-village_group\",\n            \"67\": \"E-village_group\",\n            \"68\": \"S-village_group\"\n        },\n        \"type\": \"sequence-labeling-model\",\n        \"use_crf\": true\n    },\n    \"preprocessor\": {\n        \"max_length\": 80,\n        \"model_dir\": \"damo/nlp_raner_named-entity-recognition_chinese-base-news\",\n        \"type\": \"sequence-labeling-preprocessor\"\n    },\n    \"task\": \"named-entity-recognition\",\n    \"train\": {\n        \"dataloader\": {\n            \"batch_size_per_gpu\": 16,\n            \"workers_per_gpu\": 0\n        },\n        \"hooks\": [\n            {\n                \"type\": \"TensorboardHook\"\n            },\n            {\n                \"type\": \"IterTimerHook\"\n            }\n        ],\n        \"lr_scheduler\": {\n            \"gamma\": 0.8,\n            \"step_size\": 2,\n            \"type\": \"StepLR\"\n        },\n        \"max_epochs\": 30,\n        \"optimizer\": {\n            \"lr\": 5e-05,\n            \"param_groups\": [\n                {\n                    \"lr\": 0.5,\n                    \"regex\": \"crf\"\n                }\n            ],\n            \"type\": \"AdamW\"\n        },\n        \"checkpoint\": {\n            \"best\": {\n                \"save_file_name\": \"best_model.pth\",\n                \"metric_key\": \"f1\",\n                \"save_optimizer\": false,\n                \"restore_best\": true\n            }\n        },\n        \"logging\": {\n            \"interval\": 50\n        },\n        \"work_dir\": \"experiments/transformer_crf\"\n    },\n    \"framework\": \"pytorch\"\n}\n2023-04-01 12:34:46,605 - INFO - modelscope - ===========================Training Config End===========================\n2023-04-01 12:34:46,675 - INFO - adaseq.training.default_trainer - device: cuda:0\n2023-04-01 12:34:46,678 - INFO - adaseq.training.optimizer - Done constructing parameter groups.\n2023-04-01 12:34:46,679 - INFO - adaseq.training.optimizer - Group 0: {'lr': 0.5}, [\n  \"crf.end_transitions\",\n  \"crf.start_transitions\",\n  \"crf.transitions\"\n]\n2023-04-01 12:34:46,680 - INFO - adaseq.training.optimizer - Group 1: {}, [\n  \"embedder.transformer_model.embeddings.LayerNorm.bias\",\n  \"embedder.transformer_model.embeddings.LayerNorm.weight\",\n  \"embedder.transformer_model.embeddings.position_embeddings.weight\",\n  \"embedder.transformer_model.embeddings.token_type_embeddings.weight\",\n  \"embedder.transformer_model.embeddings.word_embeddings.weight\",\n  \"embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.0.attention.self.key.bias\",\n  \"embedder.transformer_model.encoder.layer.0.attention.self.key.weight\",\n  \"embedder.transformer_model.encoder.layer.0.attention.self.query.bias\",\n  \"embedder.transformer_model.encoder.layer.0.attention.self.query.weight\",\n  \"embedder.transformer_model.encoder.layer.0.attention.self.value.bias\",\n  \"embedder.transformer_model.encoder.layer.0.attention.self.value.weight\",\n  \"embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.0.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.0.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.1.attention.self.key.bias\",\n  \"embedder.transformer_model.encoder.layer.1.attention.self.key.weight\",\n  \"embedder.transformer_model.encoder.layer.1.attention.self.query.bias\",\n  \"embedder.transformer_model.encoder.layer.1.attention.self.query.weight\",\n  \"embedder.transformer_model.encoder.layer.1.attention.self.value.bias\",\n  \"embedder.transformer_model.encoder.layer.1.attention.self.value.weight\",\n  \"embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.1.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.1.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.10.attention.self.key.bias\",\n  \"embedder.transformer_model.encoder.layer.10.attention.self.key.weight\",\n  \"embedder.transformer_model.encoder.layer.10.attention.self.query.bias\",\n  \"embedder.transformer_model.encoder.layer.10.attention.self.query.weight\",\n  \"embedder.transformer_model.encoder.layer.10.attention.self.value.bias\",\n  \"embedder.transformer_model.encoder.layer.10.attention.self.value.weight\",\n  \"embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.10.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.10.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.11.attention.self.key.bias\",\n  \"embedder.transformer_model.encoder.layer.11.attention.self.key.weight\",\n  \"embedder.transformer_model.encoder.layer.11.attention.self.query.bias\",\n  \"embedder.transformer_model.encoder.layer.11.attention.self.query.weight\",\n  \"embedder.transformer_model.encoder.layer.11.attention.self.value.bias\",\n  \"embedder.transformer_model.encoder.layer.11.attention.self.value.weight\",\n  \"embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.11.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.11.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.2.attention.self.key.bias\",\n  \"embedder.transformer_model.encoder.layer.2.attention.self.key.weight\",\n  \"embedder.transformer_model.encoder.layer.2.attention.self.query.bias\",\n  \"embedder.transformer_model.encoder.layer.2.attention.self.query.weight\",\n  \"embedder.transformer_model.encoder.layer.2.attention.self.value.bias\",\n  \"embedder.transformer_model.encoder.layer.2.attention.self.value.weight\",\n  \"embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.2.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.2.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.3.attention.self.key.bias\",\n  \"embedder.transformer_model.encoder.layer.3.attention.self.key.weight\",\n  \"embedder.transformer_model.encoder.layer.3.attention.self.query.bias\",\n  \"embedder.transformer_model.encoder.layer.3.attention.self.query.weight\",\n  \"embedder.transformer_model.encoder.layer.3.attention.self.value.bias\",\n  \"embedder.transformer_model.encoder.layer.3.attention.self.value.weight\",\n  \"embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.3.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.3.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.4.attention.self.key.bias\",\n  \"embedder.transformer_model.encoder.layer.4.attention.self.key.weight\",\n  \"embedder.transformer_model.encoder.layer.4.attention.self.query.bias\",\n  \"embedder.transformer_model.encoder.layer.4.attention.self.query.weight\",\n  \"embedder.transformer_model.encoder.layer.4.attention.self.value.bias\",\n  \"embedder.transformer_model.encoder.layer.4.attention.self.value.weight\",\n  \"embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.4.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.4.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.5.attention.self.key.bias\",\n  \"embedder.transformer_model.encoder.layer.5.attention.self.key.weight\",\n  \"embedder.transformer_model.encoder.layer.5.attention.self.query.bias\",\n  \"embedder.transformer_model.encoder.layer.5.attention.self.query.weight\",\n  \"embedder.transformer_model.encoder.layer.5.attention.self.value.bias\",\n  \"embedder.transformer_model.encoder.layer.5.attention.self.value.weight\",\n  \"embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.5.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.5.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.6.attention.self.key.bias\",\n  \"embedder.transformer_model.encoder.layer.6.attention.self.key.weight\",\n  \"embedder.transformer_model.encoder.layer.6.attention.self.query.bias\",\n  \"embedder.transformer_model.encoder.layer.6.attention.self.query.weight\",\n  \"embedder.transformer_model.encoder.layer.6.attention.self.value.bias\",\n  \"embedder.transformer_model.encoder.layer.6.attention.self.value.weight\",\n  \"embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.6.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.6.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.7.attention.self.key.bias\",\n  \"embedder.transformer_model.encoder.layer.7.attention.self.key.weight\",\n  \"embedder.transformer_model.encoder.layer.7.attention.self.query.bias\",\n  \"embedder.transformer_model.encoder.layer.7.attention.self.query.weight\",\n  \"embedder.transformer_model.encoder.layer.7.attention.self.value.bias\",\n  \"embedder.transformer_model.encoder.layer.7.attention.self.value.weight\",\n  \"embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.7.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.7.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.8.attention.self.key.bias\",\n  \"embedder.transformer_model.encoder.layer.8.attention.self.key.weight\",\n  \"embedder.transformer_model.encoder.layer.8.attention.self.query.bias\",\n  \"embedder.transformer_model.encoder.layer.8.attention.self.query.weight\",\n  \"embedder.transformer_model.encoder.layer.8.attention.self.value.bias\",\n  \"embedder.transformer_model.encoder.layer.8.attention.self.value.weight\",\n  \"embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.8.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.8.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.9.attention.self.key.bias\",\n  \"embedder.transformer_model.encoder.layer.9.attention.self.key.weight\",\n  \"embedder.transformer_model.encoder.layer.9.attention.self.query.bias\",\n  \"embedder.transformer_model.encoder.layer.9.attention.self.query.weight\",\n  \"embedder.transformer_model.encoder.layer.9.attention.self.value.bias\",\n  \"embedder.transformer_model.encoder.layer.9.attention.self.value.weight\",\n  \"embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\",\n  \"embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\",\n  \"embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\",\n  \"embedder.transformer_model.encoder.layer.9.output.dense.bias\",\n  \"embedder.transformer_model.encoder.layer.9.output.dense.weight\",\n  \"embedder.transformer_model.pooler.dense.bias\",\n  \"embedder.transformer_model.pooler.dense.weight\",\n  \"linear.bias\",\n  \"linear.weight\"\n]\n2023-04-01 12:34:46,681 - INFO - adaseq.training.optimizer - Number of trainable parameters: 102325608\n2023-04-01 12:34:46,684 - INFO - modelscope - Stage: before_run:\n    (ABOVE_NORMAL) OptimizerHook                      \n    (LOW         ) LrSchedulerHook                    \n    (LOW         ) BestCkptSaverHook                  \n    (VERY_LOW    ) AdaSeqTextLoggerHook               \n    (VERY_LOW    ) TensorboardHook                    \n -------------------- \nStage: before_train_epoch:\n    (LOW         ) LrSchedulerHook                    \n -------------------- \nStage: after_train_iter:\n    (ABOVE_NORMAL) OptimizerHook                      \n    (NORMAL      ) EvaluationHook                     \n    (LOW         ) LrSchedulerHook                    \n    (LOW         ) BestCkptSaverHook                  \n    (VERY_LOW    ) AdaSeqTextLoggerHook               \n    (VERY_LOW    ) TensorboardHook                    \n -------------------- \nStage: after_train_epoch:\n    (NORMAL      ) EvaluationHook                     \n    (LOW         ) LrSchedulerHook                    \n    (LOW         ) BestCkptSaverHook                  \n    (VERY_LOW    ) AdaSeqTextLoggerHook               \n    (VERY_LOW    ) TensorboardHook                    \n -------------------- \nStage: after_val_epoch:\n    (VERY_LOW    ) AdaSeqTextLoggerHook               \n    (VERY_LOW    ) TensorboardHook                    \n -------------------- \nStage: after_run:\n    (LOW         ) BestCkptSaverHook                  \n    (VERY_LOW    ) TensorboardHook                    \n -------------------- \n --- Hook strategies info --- \n\n --- Hook strategies info end --- \n\n2023-04-01 12:34:46,879 - INFO - modelscope - Checkpoints will be saved to experiments/transformer_crf\n2023-04-01 12:34:46,880 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: experiments/transformer_crf/metrics.json\n2023-04-01 12:34:46,881 - INFO - modelscope - tensorboard files will be saved to experiments/transformer_crf/tensorboard_output\n2023-04-01 12:35:12,276 - INFO - modelscope - epoch [1][50/554]\tlr: 5.000e-05, eta: 2:19:19, iter_time: 0.504, data_load_time: 0.082, memory: 3701, loss: 11.7582\n2023-04-01 12:35:35,392 - INFO - modelscope - epoch [1][100/554]\tlr: 5.000e-05, eta: 2:13:17, iter_time: 0.464, data_load_time: 0.086, memory: 3941, loss: 4.3025\n2023-04-01 12:36:02,375 - INFO - modelscope - epoch [1][150/554]\tlr: 5.000e-05, eta: 2:17:49, iter_time: 0.538, data_load_time: 0.097, memory: 3941, loss: 3.4796\n","output_type":"stream"}],"id":"cc9486b7-2e6d-4230-a910-cc4076147ebb"},{"cell_type":"code","source":"# process pred file to conll format\nimport os\n\nexp_dir = 'experiments/transformer_crf'\n\nwith open(f'{exp_dir}/pred.txt', 'r', encoding='utf8') as fin, \\\n     open('outputs/baseline4.pred.txt', 'w', encoding='utf8') as fout:\n    guid = 1\n    tokens = []\n    labels = []\n    for line in fin:\n        if line == '' or line == '\\n':\n            if tokens:\n                print(guid, ''.join(tokens), ' '.join(labels), sep='\\u0001', file=fout)\n                guid += 1\n                tokens = []\n                labels = []\n        else:\n            splits = line.split('\\t')\n            tokens.append(splits[0])\n            labels.append(splits[-1].rstrip())\n    if tokens:\n        print(guid, ''.join(tokens), ' '.join(labels), sep='\\u0001', file=fout)","metadata":{"ExecutionIndicator":{"show":true},"tags":[],"trusted":true},"outputs":[],"id":"29db2208-d6ed-4c69-bbfc-1c9192c7b5e9"},{"cell_type":"code","source":"","metadata":{},"outputs":[],"id":"89869dc5-a6ce-45db-b26d-1362aaf66106"}]}