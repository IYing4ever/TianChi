{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 应用BERT模型做地址解析任务(Paddle2.0)\n",
    "WGM, 202107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting paddlenlp==2.2.3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/17/9b/4535ccf0e96c302a3066bd2e4d0f44b6b1a73487c6793024475b48466c32/paddlenlp-2.2.3-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.2.3) (1.2.2)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.2.3) (0.70.11.1)\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.2.3) (2.9.0)\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.2.3) (0.4.4)\n",
      "Requirement already satisfied: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.2.3) (4.1.0)\n",
      "Requirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.2.3) (0.42.1)\n",
      "Requirement already satisfied: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp==2.2.3) (1.20.3)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp==2.2.3) (1.16.0)\n",
      "Requirement already satisfied: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp==2.2.3) (0.3.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp==2.2.3) (0.24.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.2.3) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.2.3) (1.6.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.2.3) (2.1.0)\n",
      "Installing collected packages: paddlenlp\n",
      "  Attempting uninstall: paddlenlp\n",
      "    Found existing installation: paddlenlp 2.0.7\n",
      "    Uninstalling paddlenlp-2.0.7:\n",
      "      Successfully uninstalled paddlenlp-2.0.7\n",
      "Successfully installed paddlenlp-2.2.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#安装paddlenlp2.0\r\n",
    "# !pip install pyzmq==18.1.1\r\n",
    "!pip install paddlenlp==2.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1、导入必要的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "import paddlenlp as ppnlp\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "from paddlenlp.data import Stack, Pad, Tuple\r\n",
    "from paddlenlp.metrics import ChunkEvaluator\r\n",
    "import paddle.nn.functional as F\r\n",
    "import numpy as np\r\n",
    "from functools import partial #partial()函数可以用来固定某些参数值，并返回一个新的callable对象\r\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2、数据查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "浙 B-prov\r\n",
      "江 E-prov\r\n",
      "杭 B-city\r\n",
      "州 I-city\r\n",
      "市 E-city\r\n",
      "江 B-district\r\n",
      "干 I-district\r\n",
      "区 E-district\r\n",
      "九 B-town\r\n",
      "堡 I-town\r\n"
     ]
    }
   ],
   "source": [
    "!head -n10 dataset/train.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "杭 B-city\r\n",
      "州 E-city\r\n",
      "五 B-poi\r\n",
      "洲 I-poi\r\n",
      "国 I-poi\r\n",
      "际 E-poi\r\n",
      "\r\n",
      "浙 B-prov\r\n",
      "江 I-prov\r\n",
      "省 E-prov\r\n"
     ]
    }
   ],
   "source": [
    "!head -n10  dataset/dev.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\u0001朝阳区小关北里000-0号\r\n",
      "2\u0001朝阳区惠新东街00号\r\n",
      "3\u0001朝阳区南磨房路与西大望路交口东南角\r\n",
      "4\u0001朝阳区潘家园南里00号\r\n",
      "5\u0001朝阳区向军南里二巷0号附近\r\n",
      "6\u0001朝阳区多处营业网点\r\n",
      "7\u0001朝阳区多处营业网点\r\n",
      "8\u0001朝阳区多处营业网点\r\n",
      "9\u0001朝阳区北三环中路00号商房大厦0楼\r\n",
      "10\u0001朝阳区孙河乡康营家园00区北侧底商\r\n"
     ]
    }
   ],
   "source": [
    "!head dataset/final_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3、数据格式调整，将原先每行是每个字的标注形式，修改为每行是每句话的标注形式，相邻字（标注）之间，采用符号'\\002'进行分隔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "#对文件source_filename的数据格式进行调整，结果保存在文件target_filename中\r\n",
    "def format_data(source_filename, target_filename):\r\n",
    "    #pdb.set_trace()\r\n",
    "    #结果列表初始化为空\r\n",
    "    datalist=[]\r\n",
    "    #读取source_filename所有数据到lines中，每个元素是字标注\r\n",
    "    with open(source_filename, 'r', encoding='utf-8') as f:\r\n",
    "        lines=f.readlines()\r\n",
    "    words=''\r\n",
    "    labels=''\r\n",
    "    #当前处理的是否为每句话首字符，0：是，1：不是\r\n",
    "    flag=0\r\n",
    "    #逐个处理每个字标注\r\n",
    "    for line in lines:\r\n",
    "        #空行表示每句话标注的结束\r\n",
    "        if line == '\\n':\r\n",
    "            #连接文本和标注结果\r\n",
    "            item=words+'\\t'+labels+'\\n'\r\n",
    "            #print(item)\r\n",
    "            #添加到结果列表中\r\n",
    "            datalist.append(item)\r\n",
    "            #重置文本和标注结果\r\n",
    "            words=''\r\n",
    "            labels=''\r\n",
    "            flag=0\r\n",
    "            continue\r\n",
    "        #pdb.set_trace()\r\n",
    "        #分离出字和标注\r\n",
    "        word, label = line.strip('\\n').split(' ')\r\n",
    "        #不是每句话的首字符\r\n",
    "        if flag==1:\r\n",
    "            #words/labels非空，和字/标签连接时需要添加分隔符'\\002'\r\n",
    "            words=words+'\\002'+word\r\n",
    "            labels=labels+'\\002'+label\r\n",
    "        else:#每句话首字符，words/labels为空，和字/标签连接时不需要添加分隔符'\\002'\r\n",
    "            words=words+word\r\n",
    "            labels=labels+label\r\n",
    "            flag=1#修改标志\r\n",
    "    with open(target_filename, 'w', encoding='utf-8') as f:\r\n",
    "        #pdb.set_trace()\r\n",
    "        #将转换结果写入文件target_filename\r\n",
    "        lines=f.writelines(datalist)\r\n",
    "    print(f'{source_filename}文件格式转换完毕，保存为{target_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/dev.conll文件格式转换完毕，保存为./dataset/dev.txt\n",
      "./dataset/train.conll文件格式转换完毕，保存为./dataset/train.txt\n"
     ]
    }
   ],
   "source": [
    "#逐个转换文件\r\n",
    "format_data('./dataset/dev.conll', './dataset/dev.txt')\r\n",
    "format_data(r'./dataset/train.conll', r'./dataset/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "杭\u0002州\u0002五\u0002洲\u0002国\u0002际\tB-city\u0002E-city\u0002B-poi\u0002I-poi\u0002I-poi\u0002E-poi\r\n",
      "浙\u0002江\u0002省\u0002杭\u0002州\u0002市\u0002余\u0002杭\u0002乔\u0002司\u0002街\u0002道\u0002博\u0002卡\u0002路\u00020\u0002号\u0002博\u0002卡\u0002制\u0002衣\tB-prov\u0002I-prov\u0002E-prov\u0002B-city\u0002I-city\u0002E-city\u0002B-district\u0002E-district\u0002B-town\u0002I-town\u0002I-town\u0002E-town\u0002B-road\u0002I-road\u0002E-road\u0002B-roadno\u0002E-roadno\u0002B-poi\u0002I-poi\u0002I-poi\u0002E-poi\r\n",
      "浙\u0002江\u0002诸\u0002暨\u0002市\u0002暨\u0002阳\u0002八\u0002一\u0002新\u0002村\u00020\u00020\u0002幢\tB-prov\u0002E-prov\u0002B-district\u0002I-district\u0002E-district\u0002B-town\u0002E-town\u0002B-poi\u0002I-poi\u0002I-poi\u0002E-poi\u0002B-houseno\u0002I-houseno\u0002E-houseno\r\n",
      "杭\u0002州\u0002市\u0002武\u0002林\u0002广\u0002场\u0002杭\u0002州\u0002大\u0002厦\u0002商\u0002城\u0002A\u0002座\u0002九\u0002层\tB-city\u0002I-city\u0002E-city\u0002B-poi\u0002I-poi\u0002I-poi\u0002E-poi\u0002B-subpoi\u0002I-subpoi\u0002I-subpoi\u0002E-subpoi\u0002B-subpoi\u0002E-subpoi\u0002B-houseno\u0002E-houseno\u0002B-floorno\u0002E-floorno\r\n",
      "浙\u0002江\u0002省\u0002杭\u0002州\u0002市\u0002拱\u0002墅\u0002区\u0002登\u0002云\u0002路\u00020\u00020\u00020\u00020\u0002号\u0002时\u0002代\u0002电\u0002子\u0002市\u0002场\tB-prov\u0002I-prov\u0002E-prov\u0002B-city\u0002I-city\u0002E-city\u0002B-district\u0002I-district\u0002E-district\u0002B-road\u0002I-road\u0002E-road\u0002B-roadno\u0002I-roadno\u0002I-roadno\u0002I-roadno\u0002E-roadno\u0002B-poi\u0002I-poi\u0002I-poi\u0002I-poi\u0002I-poi\u0002E-poi\r\n",
      "浙\u0002江\u0002省\u0002宁\u0002波\u0002市\u0002慈\u0002溪\u0002市\u0002宗\u0002汉\u0002街\u0002道\u0002联\u0002丰\u0002公\u0002寓\u00020\u00020\u0002栋\tB-prov\u0002I-prov\u0002E-prov\u0002B-city\u0002I-city\u0002E-city\u0002B-district\u0002I-district\u0002E-district\u0002B-town\u0002I-town\u0002I-town\u0002E-town\u0002B-poi\u0002I-poi\u0002I-poi\u0002E-poi\u0002B-houseno\u0002I-houseno\u0002E-houseno\r\n",
      "浙\u0002江\u0002省\u0002温\u0002州\u0002市\u0002鹿\u0002城\u0002区\u0002劳\u0002务\u0002市\u0002场\u0002跨\u0002境\u0002电\u0002商\u0002园\u00020\u00020\u0002楼\u0002艺\u0002网\u0002科\u0002技\u0002有\u0002限\u0002公\u0002司\tB-prov\u0002I-prov\u0002E-prov\u0002B-city\u0002I-city\u0002E-city\u0002B-district\u0002I-district\u0002E-district\u0002B-poi\u0002I-poi\u0002I-poi\u0002E-poi\u0002B-devzone\u0002I-devzone\u0002I-devzone\u0002I-devzone\u0002E-devzone\u0002B-floorno\u0002I-floorno\u0002E-floorno\u0002B-subpoi\u0002I-subpoi\u0002I-subpoi\u0002I-subpoi\u0002I-subpoi\u0002I-subpoi\u0002I-subpoi\u0002E-subpoi\r\n",
      "康\u0002中\u0002路\u00020\u00020\u0002号\u0002康\u0002城\u0002工\u0002业\u0002园\u00020\u00020\u0002幢\u00020\u0002楼\tB-road\u0002I-road\u0002E-road\u0002B-roadno\u0002I-roadno\u0002E-roadno\u0002B-devzone\u0002I-devzone\u0002I-devzone\u0002I-devzone\u0002E-devzone\u0002B-houseno\u0002I-houseno\u0002E-houseno\u0002B-floorno\u0002E-floorno\r\n",
      "金\u0002华\u0002永\u0002康\u0002市\u0002城\u0002西\u0002工\u0002业\u0002区\u0002蓝\u0002天\u0002路\u0002坊\u0002培\u0002电\u0002脑\tB-city\u0002E-city\u0002B-district\u0002I-district\u0002E-district\u0002B-devzone\u0002I-devzone\u0002I-devzone\u0002I-devzone\u0002E-devzone\u0002B-road\u0002I-road\u0002E-road\u0002B-poi\u0002I-poi\u0002I-poi\u0002E-poi\r\n",
      "宜\u0002山\u0002人\u0002民\u0002路\u00020\u00020\u00020\u00020\u0002号\u0002后\u0002栋\u0002纸\u0002巾\u0002厂\tB-town\u0002E-town\u0002B-road\u0002I-road\u0002E-road\u0002B-roadno\u0002I-roadno\u0002I-roadno\u0002I-roadno\u0002E-roadno\u0002B-houseno\u0002E-houseno\u0002B-poi\u0002I-poi\u0002E-poi\r\n"
     ]
    }
   ],
   "source": [
    "!head dataset/dev.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4、构建Label标签表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#提取文件source_filename1和source_filename2的标签类型，保存到target_filename\r\n",
    "def gernate_dic(source_filename1, source_filename2, target_filename):\r\n",
    "    #标签类型列表初始化为空\r\n",
    "    data_list=[]\r\n",
    "\r\n",
    "    #读取source_filename1所有行到lines中，每行元素是单字和标注\r\n",
    "    with open(source_filename1, 'r', encoding='utf-8') as f:\r\n",
    "        lines=f.readlines()\r\n",
    "\r\n",
    "    #处理每行数据（单字+‘ ’+标注）\r\n",
    "    for line in lines:\r\n",
    "        #数据非空\r\n",
    "        if line != '\\n':\r\n",
    "            #提取标注，-1是数组最后1个元素\r\n",
    "            dic=line.strip('\\n').split(' ')[-1]\r\n",
    "            #不在标签类型列表中，则添加\r\n",
    "            if dic+'\\n' not in data_list:\r\n",
    "                data_list.append(dic+'\\n')\r\n",
    "    \r\n",
    "    #读取source_filename2所有行到lines中，每行元素是单字和标注\r\n",
    "    with open(source_filename2, 'r', encoding='utf-8') as f:\r\n",
    "        lines=f.readlines()\r\n",
    "\r\n",
    "    #处理每行数据（单字+‘ ’+标注）\r\n",
    "    for line in lines:\r\n",
    "        #数据非空\r\n",
    "        if line != '\\n':\r\n",
    "            #提取标注，-1是数组最后1个元素\r\n",
    "            dic=line.strip('\\n').split(' ')[-1]\r\n",
    "            #不在标签类型列表中，则添加\r\n",
    "            if dic+'\\n' not in data_list:\r\n",
    "                data_list.append(dic+'\\n')\r\n",
    "\r\n",
    "    with open(target_filename, 'w', encoding='utf-8') as f:\r\n",
    "        #将标签类型列表写入文件target_filename\r\n",
    "        lines=f.writelines(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-prov\r\n",
      "E-prov\r\n",
      "B-city\r\n",
      "I-city\r\n",
      "E-city\r\n",
      "B-district\r\n",
      "I-district\r\n",
      "E-district\r\n",
      "B-town\r\n",
      "I-town\r\n",
      "E-town\r\n",
      "B-community\r\n",
      "I-community\r\n",
      "E-community\r\n",
      "B-poi\r\n",
      "E-poi\r\n",
      "I-prov\r\n",
      "I-poi\r\n",
      "B-road\r\n",
      "E-road\r\n",
      "B-roadno\r\n",
      "I-roadno\r\n",
      "E-roadno\r\n",
      "I-road\r\n",
      "O\r\n",
      "B-subpoi\r\n",
      "I-subpoi\r\n",
      "E-subpoi\r\n",
      "B-devzone\r\n",
      "I-devzone\r\n",
      "E-devzone\r\n",
      "B-houseno\r\n",
      "I-houseno\r\n",
      "E-houseno\r\n",
      "B-intersection\r\n",
      "I-intersection\r\n",
      "E-intersection\r\n",
      "B-assist\r\n",
      "I-assist\r\n",
      "E-assist\r\n",
      "B-cellno\r\n",
      "I-cellno\r\n",
      "E-cellno\r\n",
      "B-floorno\r\n",
      "E-floorno\r\n",
      "S-assist\r\n",
      "I-floorno\r\n",
      "B-distance\r\n",
      "I-distance\r\n",
      "E-distance\r\n",
      "B-village_group\r\n",
      "E-village_group\r\n",
      "I-village_group\r\n",
      "S-poi\r\n",
      "S-intersection\r\n",
      "S-district\r\n",
      "S-community\r\n"
     ]
    }
   ],
   "source": [
    "# 根据训练集和验证集生成dic，保存所有的标签\r\n",
    "gernate_dic('dataset/train.conll', 'dataset/dev.conll', 'dataset/mytag.dic')\r\n",
    "# 查看生成的dic文件\r\n",
    "!cat dataset/mytag.dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5、加载自定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加载数据文件datafiles\r\n",
    "def load_dataset(datafiles):\r\n",
    "    #读取数据文件data_path\r\n",
    "    def read(data_path):\r\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\r\n",
    "            next(fp)  # Skip header  #Deleted by WGM\r\n",
    "            #处理每行数据（文本+‘\\t’+标注）\r\n",
    "            for line in fp.readlines():\r\n",
    "                #提取文本和标注\r\n",
    "                words, labels = line.strip('\\n').split('\\t')\r\n",
    "                #文本中单字和标注构成的数组\r\n",
    "                words = words.split('\\002')\r\n",
    "                labels = labels.split('\\002')\r\n",
    "                #迭代返回文本和标注\r\n",
    "                yield words, labels\r\n",
    "    \r\n",
    "    #根据datafiles的数据类型，选择合适的处理方式\r\n",
    "    if isinstance(datafiles, str):#字符串，单个文件名称\r\n",
    "        #返回单个文件对应的单个数据集\r\n",
    "        return MapDataset(list(read(datafiles)))\r\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):#列表或元组，多个文件名称\r\n",
    "        #返回多个文件对应的多个数据集\r\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]\r\n",
    "\r\n",
    "#加载字典文件，文件由单列构成，需要设置value\r\n",
    "def load_dict_single(dict_path):\r\n",
    "    #字典初始化为空\r\n",
    "    vocab = {}\r\n",
    "    #value是自增数值，从0开始\r\n",
    "    i = 0\r\n",
    "    #逐行读取字典文件\r\n",
    "    for line in open(dict_path, 'r', encoding='utf-8'):\r\n",
    "        #将每行文字设置为key\r\n",
    "        key = line.strip('\\n')\r\n",
    "        #设置对应的value\r\n",
    "        vocab[key] = i\r\n",
    "        i+=1\r\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集、验证集、测试集的数量：\n",
      "8854 1969\n",
      "(['浙', '江', '省', '温', '州', '市', '平', '阳', '县', '海', '西', '镇', '宋', '埠', '公', '园', '南', '路', '0', '0', '0', '0', '号'], ['B-prov', 'I-prov', 'E-prov', 'B-city', 'I-city', 'E-city', 'B-district', 'I-district', 'E-district', 'B-town', 'I-town', 'E-town', 'B-poi', 'I-poi', 'I-poi', 'E-poi', 'B-road', 'E-road', 'B-roadno', 'I-roadno', 'I-roadno', 'I-roadno', 'E-roadno'])\n",
      "(['浙', '江', '省', '杭', '州', '市', '余', '杭', '乔', '司', '街', '道', '博', '卡', '路', '0', '号', '博', '卡', '制', '衣'], ['B-prov', 'I-prov', 'E-prov', 'B-city', 'I-city', 'E-city', 'B-district', 'E-district', 'B-town', 'I-town', 'I-town', 'E-town', 'B-road', 'I-road', 'E-road', 'B-roadno', 'E-roadno', 'B-poi', 'I-poi', 'I-poi', 'E-poi'])\n",
      "{'B-prov': 0, 'E-prov': 1, 'B-city': 2, 'I-city': 3, 'E-city': 4, 'B-district': 5, 'I-district': 6, 'E-district': 7, 'B-town': 8, 'I-town': 9, 'E-town': 10, 'B-community': 11, 'I-community': 12, 'E-community': 13, 'B-poi': 14, 'E-poi': 15, 'I-prov': 16, 'I-poi': 17, 'B-road': 18, 'E-road': 19, 'B-roadno': 20, 'I-roadno': 21, 'E-roadno': 22, 'I-road': 23, 'O': 24, 'B-subpoi': 25, 'I-subpoi': 26, 'E-subpoi': 27, 'B-devzone': 28, 'I-devzone': 29, 'E-devzone': 30, 'B-houseno': 31, 'I-houseno': 32, 'E-houseno': 33, 'B-intersection': 34, 'I-intersection': 35, 'E-intersection': 36, 'B-assist': 37, 'I-assist': 38, 'E-assist': 39, 'B-cellno': 40, 'I-cellno': 41, 'E-cellno': 42, 'B-floorno': 43, 'E-floorno': 44, 'S-assist': 45, 'I-floorno': 46, 'B-distance': 47, 'I-distance': 48, 'E-distance': 49, 'B-village_group': 50, 'E-village_group': 51, 'I-village_group': 52, 'S-poi': 53, 'S-intersection': 54, 'S-district': 55, 'S-community': 56}\n"
     ]
    }
   ],
   "source": [
    "# 加载Bert模型需要的输入数据\r\n",
    "train_ds, dev_ds = load_dataset(datafiles=(\r\n",
    "        './dataset/train.txt', './dataset/dev.txt'))\r\n",
    "#加载标签文件，并转换为KV表，K为标签，V为编号（从0开始递增）\r\n",
    "label_vocab = load_dict_single('./dataset/mytag.dic')\r\n",
    "\r\n",
    "print(\"训练集、验证集、测试集的数量：\")\r\n",
    "print(len(train_ds),len(dev_ds))\r\n",
    "print(train_ds[0])\r\n",
    "print(dev_ds[0])\r\n",
    "print(label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6、数据集预处理，转换为Bert模型可以接受的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#数据预处理\r\n",
    "#tokenizer：预编码器，label_vocab：标签类型KV表，K是标签类型，V是编码\r\n",
    "def convert_example(example,tokenizer,label_vocab,max_seq_length=256,is_test=False):\r\n",
    "    #测试集没有标签\r\n",
    "    if is_test:\r\n",
    "        text = example\r\n",
    "    else:#训练集和验证集包含标签\r\n",
    "        text, label = example\r\n",
    "    #tokenizer.encode方法能够完成切分token，映射token ID以及拼接特殊token\r\n",
    "    encoded_inputs = tokenizer.encode(text=text, max_seq_len=None, pad_to_max_seq_len=False, return_length=True)\r\n",
    "    #pdb.set_trace()\r\n",
    "    #获取字符编码（'input_ids'）、类型编码（'token_type_ids'）、字符串长度（'seq_len'）\r\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\r\n",
    "    segment_ids = encoded_inputs[\"token_type_ids\"]\r\n",
    "    seq_len = encoded_inputs[\"seq_len\"]\r\n",
    "\r\n",
    "    if not is_test:#训练集和验证集\r\n",
    "        #[CLS]和[SEP]对应的标签均是['O']，添加到标签序列中\r\n",
    "        label = ['O'] + label + ['O']\r\n",
    "        #生成由标签编码构成的序列\r\n",
    "        label = [label_vocab[x] for x in label]\r\n",
    "        return input_ids, segment_ids, seq_len, label\r\n",
    "    else:#测试集，不返回标签序列\r\n",
    "        return input_ids, segment_ids, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-04 14:04:04,080] [    INFO] - Downloading https://bj.bcebos.com/paddle-hapi/models/bert/bert-base-chinese-vocab.txt and saved to /home/aistudio/.paddlenlp/models/bert-base-chinese\n",
      "[2023-04-04 14:04:04,084] [    INFO] - Downloading bert-base-chinese-vocab.txt from https://bj.bcebos.com/paddle-hapi/models/bert/bert-base-chinese-vocab.txt\n",
      "100%|██████████| 107k/107k [00:00<00:00, 3.11MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([101, 3851, 3736, 4689, 3946, 2336, 2356, 2398, 7345, 1344, 3862, 6205, 7252, 2129, 1819, 1062, 1736, 1298, 6662, 121, 121, 121, 121, 1384, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 25, [24, 0, 16, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 17, 17, 15, 18, 19, 20, 21, 21, 21, 22, 24])]\n"
     ]
    }
   ],
   "source": [
    "#加载Bert预训练模型，将原始输入文本转化成序列标注模型model可接受的输入数据格式。\r\n",
    "tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(\"bert-base-chinese\")\r\n",
    "#functools.partial()的功能：预先设置参数，减少使用时设置的参数个数\r\n",
    "#使用partial()来固定convert_example函数的tokenizer, label_vocab, max_seq_length等参数值\r\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, label_vocab=label_vocab, max_seq_length=128)\r\n",
    "\r\n",
    "\r\n",
    "#对训练集和测试集进行编码\r\n",
    "train_ds.map(trans_func)\r\n",
    "dev_ds.map(trans_func)\r\n",
    "print ([train_ds[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#使用paddle.io.DataLoader接口多线程异步加载数据。\r\n",
    "ignore_label = -1\r\n",
    "#创建Tuple对象，将多个批处理函数的处理结果连接在一起\r\n",
    "#因为数据集train_ds、dev_ds的每条数据包含4部分，所以Tuple对象中包含4个批处理函数\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    #将每条数据的input_ids组合为数组，如果input_ids不等长，那么填充为pad_val\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "    #将每条数据的segment_ids组合为数组，如果segment_ids不等长，那么填充为pad_val\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\r\n",
    "    #将每条数据的seq_len组合为数组\r\n",
    "    Stack(),\r\n",
    "    #将每条数据的label组合为数组，如果label不等长，那么填充为pad_val\r\n",
    "    Pad(axis=0, pad_val=ignore_label)\r\n",
    "): fn(samples)\r\n",
    "\r\n",
    "#paddle.io.DataLoader加载给定数据集，返回迭代器，每次迭代访问batch_size条数据\r\n",
    "#使用collate_fn定义所读取数据的格式\r\n",
    "#训练集\r\n",
    "train_loader = paddle.io.DataLoader(\r\n",
    "    dataset=train_ds,\r\n",
    "    batch_size=300,\r\n",
    "    return_list=True,\r\n",
    "    collate_fn=batchify_fn)\r\n",
    "#验证集\r\n",
    "dev_loader = paddle.io.DataLoader(\r\n",
    "    dataset=dev_ds,\r\n",
    "    batch_size=300,\r\n",
    "    return_list=True,\r\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 7、Bert模型加载和训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-04 14:04:04,860] [    INFO] - Downloading http://bj.bcebos.com/paddlenlp/models/transformers/bert/bert-base-chinese.pdparams and saved to /home/aistudio/.paddlenlp/models/bert-base-chinese\n",
      "[2023-04-04 14:04:04,863] [    INFO] - Downloading bert-base-chinese.pdparams from http://bj.bcebos.com/paddlenlp/models/transformers/bert/bert-base-chinese.pdparams\n",
      " 15%|█▍        | 102M/680M [00:12<04:22, 2.31MB/s] "
     ]
    }
   ],
   "source": [
    "#加载用于token分类任务的Fine-tune网络BertForTokenClassification。\r\n",
    "model = ppnlp.transformers.BertForTokenClassification.from_pretrained(\"bert-base-chinese\", num_classes=len(label_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#设置Fine-Tune优化策略\r\n",
    "#计算了块检测的精确率、召回率和F1-score。常用于序列标记任务，如命名实体识别\r\n",
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\r\n",
    "#交叉熵损失函数\r\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\r\n",
    "#在Adam的基础上加入了权重衰减的优化器，可以解决L2正则化失效问题\r\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#评估函数\r\n",
    "def evaluate(model, metric, data_loader):\r\n",
    "    model.eval()\r\n",
    "    metric.reset()#评估器置位\r\n",
    "    #依次处理每批数据\r\n",
    "    for input_ids, seg_ids, lens, labels in data_loader:\r\n",
    "        #单字属于不同标签的概率\r\n",
    "        logits = model(input_ids, seg_ids)\r\n",
    "        #损失函数的平均值\r\n",
    "        loss = paddle.mean(criterion(logits, labels))\r\n",
    "        #按照概率最大原则，计算单字的标签编号\r\n",
    "        #argmax计算logits中最大元素值的索引，从0开始\r\n",
    "        preds = paddle.argmax(logits, axis=-1)\r\n",
    "        #推理块个数，标签个数，正确的标签个数\r\n",
    "        n_infer, n_label, n_correct = metric.compute(None, lens, preds, labels)\r\n",
    "        #更新评估的参数\r\n",
    "        metric.update(n_infer.numpy(), n_label.numpy(), n_correct.numpy())\r\n",
    "        #平均化的准确率、召回率、F1值\r\n",
    "        precision, recall, f1_score = metric.accumulate()\r\n",
    "    print(\"评估准确度: %.6f - 召回率: %.6f - f1得分: %.6f- 损失函数: %.6f\" % (precision, recall, f1_score, loss))\r\n",
    "    model.train()\r\n",
    "    metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#模型训练\r\n",
    "global_step = 0\r\n",
    "for epoch in range(5):\r\n",
    "    #依次处理每批数据\r\n",
    "    for step, (input_ids, segment_ids, seq_lens, labels) in enumerate(train_loader, start=1):\r\n",
    "        #单字属于不同标签的概率\r\n",
    "        logits = model(input_ids, segment_ids)\r\n",
    "        #按照概率最大原则，计算单字的标签编号\r\n",
    "        preds = paddle.argmax(logits, axis=-1)\r\n",
    "        #推理块个数，标签个数，正确的标签个数\r\n",
    "        n_infer, n_label, n_correct = metric.compute(None, seq_lens, preds, labels)\r\n",
    "        #更新评估的参数\r\n",
    "        metric.update(n_infer.numpy(), n_label.numpy(), n_correct.numpy())\r\n",
    "        #平均化的准确率、召回率、F1值\r\n",
    "        precision, recall, f1_score = metric.accumulate()\r\n",
    "        #损失函数的平均值\r\n",
    "        loss = paddle.mean(criterion(logits, labels))\r\n",
    "        #回传损失函数，计算梯度\r\n",
    "        loss.backward()\r\n",
    "\r\n",
    "        if global_step % 10 == 0 :\r\n",
    "            print(\"训练集的当前epoch:%d - step:%d\" % (epoch, step))\r\n",
    "            print(\"训练准确度: %.6f, 召回率: %.6f, f1得分: %.6f- 损失函数: %.6f\" % (precision, recall, f1_score, loss))\r\n",
    "        \r\n",
    "        #根据梯度来更新参数\r\n",
    "        optimizer.step()\r\n",
    "        #梯度置零\r\n",
    "        optimizer.clear_grad()\r\n",
    "        global_step += 1\r\n",
    "    #评估训练模型\r\n",
    "    evaluate(model, metric, dev_loader)\r\n",
    "    paddle.save(model.state_dict(),\r\n",
    "            './checkpoint/model_%d.pdparams'  % (global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#模型存储\r\n",
    "!mkdir bert_result\r\n",
    "model.save_pretrained('./bert_result')\r\n",
    "tokenizer.save_pretrained('./bert_result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 8、加载和处理测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加载测试数据\r\n",
    "def load_testdata(datafiles):\r\n",
    "    def read(data_path):\r\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\r\n",
    "            # next(fp)  # 没有header，不用Skip header\r\n",
    "            for line in fp.readlines():\r\n",
    "                ids, words = line.strip('\\n').split('\\001')\r\n",
    "                # 要预测的数据集没有label，伪造个O，不知道可以不 ，应该后面预测不会用label\r\n",
    "                labels=['O' for x in range(0,len(words))]\r\n",
    "                words_array=[]\r\n",
    "                for c in words:\r\n",
    "                    words_array.append(c)\r\n",
    "                yield words_array, labels\r\n",
    "    \r\n",
    "    #根据datafiles的数据类型，选择合适的处理方式\r\n",
    "    if isinstance(datafiles, str):#字符串，单个文件名称\r\n",
    "        #返回单个文件对应的单个数据集\r\n",
    "        return MapDataset(list(read(datafiles)))\r\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):#列表或元组，多个文件名称、\r\n",
    "        #返回多个文件对应的多个数据集\r\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加载测试文件\r\n",
    "test_ds = load_testdata(datafiles=('./dataset/final_test.txt'))\r\n",
    "for i in range(10):\r\n",
    "    print(test_ds[i])\r\n",
    "#预处理编码\r\n",
    "test_ds.map(trans_func)\r\n",
    "print (test_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#使用paddle.io.DataLoader接口多线程异步加载数据。\r\n",
    "ignore_label = 1\r\n",
    "#创建Tuple对象，将多个批处理函数的处理结果连接在一起\r\n",
    "#因为数据集train_ds、dev_ds的每条数据包含4部分，所以Tuple对象中包含4个批处理函数，分别对应Token ID、Token Type、Len、Label\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "    Stack(),  # seq_len\r\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\r\n",
    "): fn(samples)\r\n",
    "#paddle.io.DataLoader加载给定数据集，返回迭代器，每次迭代访问batch_size条数据\r\n",
    "#使用collate_fn定义所读取数据的格式\r\n",
    "test_loader = paddle.io.DataLoader(\r\n",
    "    dataset=test_ds,\r\n",
    "    batch_size=50,\r\n",
    "    return_list=True,\r\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 9、Bert模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#将标签编码转换为标签名称，组合成预测结果\r\n",
    "#ds：Bert模型生成的编码序列列表，decodes：待转换的标签编码列表，lens：句子有效长度列表，label_vocab：标签类型KV表\r\n",
    "def wgm_trans_decodes(ds, decodes, lens, label_vocab):\r\n",
    "    #将decodes和lens由列表转换为数组\r\n",
    "    decodes = [x for batch in decodes for x in batch]\r\n",
    "    lens = [x for batch in lens for x in batch]\r\n",
    "    #先使用zip形成元祖（编号, 标签），然后使用dict形成字典\r\n",
    "    id_label = dict(zip(label_vocab.values(), label_vocab.keys()))\r\n",
    "    #保存所有句子解析结果的列表\r\n",
    "    results=[]\r\n",
    "    #初始化编号\r\n",
    "    inNum = 1;\r\n",
    "    #逐个处理待转换的标签编码列表\r\n",
    "    for idx, end in enumerate(lens):\r\n",
    "        #句子单字构成的数组\r\n",
    "        sent_array = ds.data[idx][0][:end]\r\n",
    "        #句子单字标签构成的数组\r\n",
    "        tags_array = [id_label[x] for x in decodes[idx][1:end]]\r\n",
    "        #初始化句子和解析结果\r\n",
    "        sent = \"\";\r\n",
    "        tags = \"\";\r\n",
    "        #将字符串数组转换为单个字符串\r\n",
    "        for i in range(end-2):\r\n",
    "            #pdb.set_trace()\r\n",
    "            #单字直接连接，形成句子\r\n",
    "            sent = sent + sent_array[i]\r\n",
    "            #标签以空格连接\r\n",
    "            if i > 0:\r\n",
    "                tags = tags + \" \" + tags_array[i]\r\n",
    "            else:#第1个标签\r\n",
    "                tags = tags_array[i]\r\n",
    "        #构成结果串：编号+句子+标签序列，中间用“\\u0001”连接\r\n",
    "        current_pred = str(inNum) + '\\u0001' + sent + '\\u0001' + tags + \"\\n\"\r\n",
    "        #pdb.set_trace()\r\n",
    "        #添加到句子解析结果的列表\r\n",
    "        results.append(current_pred)\r\n",
    "        inNum = inNum + 1\r\n",
    "    return results\r\n",
    "\r\n",
    "#从标签编码中提取出地址元素\r\n",
    "#ds：ERNIE模型生成的编码序列列表，decodes：待转换的标签编码列表，lens：句子有效长度列表，label_vocab：标签类型KV表\r\n",
    "def wgm_parse_decodes(ds, decodes, lens, label_vocab):\r\n",
    "    #将decodes和lens由列表转换为数组\r\n",
    "    decodes = [x for batch in decodes for x in batch]\r\n",
    "    lens = [x for batch in lens for x in batch]\r\n",
    "    #先使用zip形成元祖（编号, 标签），然后使用dict形成字典\r\n",
    "    id_label = dict(zip(label_vocab.values(), label_vocab.keys()))\r\n",
    "    \r\n",
    "    #地址元素提取结果，每行是单个句子的地址元素列表\r\n",
    "    #例如：('朝阳区', 'district') ('小关北里', 'poi') ('000-0号', 'houseno')\r\n",
    "    outputs = []\r\n",
    "    for idx, end in enumerate(lens):\r\n",
    "        #句子单字构成的数组\r\n",
    "        sent = ds.data[idx][0][:end]\r\n",
    "        #句子单字标签构成的数组\r\n",
    "        tags = [id_label[x] for x in decodes[idx][1:end]]\r\n",
    "        #初始化地址元素名称和标签列表\r\n",
    "        sent_out = []\r\n",
    "        tags_out = []\r\n",
    "        #当前解析出来的地址元素名称\r\n",
    "        words = \"\"\r\n",
    "        #pdb.set_trace()\r\n",
    "        #逐个处理（单字, 标签）\r\n",
    "        #提取原理：如果当前标签是O，或者以B开头，那么说明遇到新的地址元素，需要存储已经解析出来的地址元素名称words\r\n",
    "        #然后，根据情况进行处理\r\n",
    "        for s, t in zip(sent, tags):\r\n",
    "            if t.startswith('B-') or t == 'O':#遇到新的地址元素\r\n",
    "                if len(words):#words非空，需要存储到sent_out\r\n",
    "                    sent_out.append(words)\r\n",
    "                if t == 'O':#标签为O，则直接存储标签\r\n",
    "                    #pdb.set_trace()\r\n",
    "                    tags_out.append(t)\r\n",
    "                else:#提取出标签\r\n",
    "                    tags_out.append(t.split('-')[1])\r\n",
    "                #新地址元素名称首字符\r\n",
    "                words = s\r\n",
    "            else:#完善地址元素名称\r\n",
    "                words += s\r\n",
    "        #处理地址串第1个地址元素时，sent_out长度为0，和tags_out的长度不同，需要补齐\r\n",
    "        if len(sent_out) < len(tags_out):\r\n",
    "            sent_out.append(words)\r\n",
    "        #按照（名称,标签）的形式组织地址元素，并且用空格分隔开\r\n",
    "        outputs.append(' '.join(\r\n",
    "            [str((s, t)) for s, t in zip(sent_out, tags_out)]))\r\n",
    "        #换行符号\r\n",
    "        outputs.append('\\n')\r\n",
    "    return outputs\r\n",
    "\r\n",
    "#使用Bert模型推理，并保存预测结果\r\n",
    "#data_loader：\r\n",
    "def wgm_predict_save(model, data_loader, ds, label_vocab, tagged_filename, element_filename):\r\n",
    "    pred_list = []\r\n",
    "    len_list = []\r\n",
    "    for input_ids, seg_ids, lens, labels in data_loader:\r\n",
    "        #pdb.set_trace()\r\n",
    "        logits = model(input_ids, seg_ids)\r\n",
    "        pred = paddle.argmax(logits, axis=-1)\r\n",
    "        #print(pred)\r\n",
    "        pred_list.append(pred.numpy())\r\n",
    "        len_list.append(lens.numpy())\r\n",
    "    #将标签编码转换为标签名称，组合成预测结果\r\n",
    "    predlist = wgm_trans_decodes(ds, pred_list, len_list, label_vocab)\r\n",
    "    #从标签编码中提取出地址元素\r\n",
    "    elemlist = wgm_parse_decodes(ds, pred_list, len_list, label_vocab)\r\n",
    "    #保存预测结果\r\n",
    "    with open(tagged_filename, 'w', encoding='utf-8') as f:\r\n",
    "        f.writelines(predlist)\r\n",
    "    #保存地址元素\r\n",
    "    with open(element_filename, 'w', encoding='utf-8') as f:\r\n",
    "        f.writelines(elemlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加载Bert模型\r\n",
    "model = ppnlp.transformers.BertForTokenClassification.from_pretrained(\"bert-base-chinese\", num_classes=len(label_vocab))\r\n",
    "model_dict = paddle.load('bert_result/model_state.pdparams')\r\n",
    "model.set_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#推理并预测结果\r\n",
    "wgm_predict_save(model, test_loader, test_ds, label_vocab, \"predict_wgm.txt\", \"element_wgm.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
